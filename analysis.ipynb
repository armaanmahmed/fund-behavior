{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13F Holdings Analysis: AI Stocks\n",
    "\n",
    "This notebook analyzes institutional investor behavior in AI stocks through SEC 13F filings.\n",
    "\n",
    "**Key questions:**\n",
    "- Which hedge funds have the largest holdings in AI stocks?\n",
    "- How has institutional ownership of AI stocks evolved over time?\n",
    "- Which specific AI stocks do top funds favor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import pandas as pd\n",
    "    from pathlib import Path\n",
    "    from ai_stocks import AI_STOCKS, ALL_AI_TICKERS\n",
    "    import json\n",
    "    print('Packages and dependencies loaded successfully.')\n",
    "except ImportError as e:\n",
    "    print(f'Failed to load package or dependency: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load 13F Data\n",
    "\n",
    "SEC 13F filings are distributed as bulk TSV files. Quarters are stored in directories like `data/raw/2023q4_form13f/` with three key files:\n",
    "- `INFOTABLE.tsv` - holdings (CUSIP, value, shares)\n",
    "- `COVERPAGE.tsv` - filer metadata (fund name, filing date)\n",
    "- `SUBMISSION.tsv` - submission data (type of filing, relevant period)\n",
    "\n",
    "First, we build a ticker lookup dictionary using the provided JSON file. From this, we create a lookup dictionary specifically for AI stock tickers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"data/raw\")\n",
    "\n",
    "with open('data/reference/company-tickers.json', 'r') as ticker_map_file:\n",
    "    sec_tickers = json.load(ticker_map_file)\n",
    "\n",
    "    # the file is stored as a dictionary of numbers to mappings: \n",
    "    # {\"0\":{\"cik_str\":1045810,\"ticker\":\"NVDA\",\"title\":\"NVIDIA CORP\"},\"1\":{\"cik_str\":320193,\"ticker\":\"AAPL\"}, ...}\n",
    "\n",
    "    # build a dictionary from the company name to the ticker symbol\n",
    "    name_to_ticker = {entry['title'].upper(): entry['ticker'] for entry in sec_tickers.values()}\n",
    "\n",
    "# crucial for ticker matching that occurs in the later stages: reduce matching from ~10,000 entries to ~50\n",
    "ai_name_to_ticker = {name: ticker for name, ticker in name_to_ticker.items() if ticker in ALL_AI_TICKERS}\n",
    "\n",
    "# build reverse mapping: ticker -> AI category\n",
    "ticker_to_ai_type = {ticker: category for category, tickers in AI_STOCKS.items() for ticker in tickers}\n",
    "\n",
    "ai_name_to_ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_quarters():\n",
    "    \"\"\"List available quarters of 13F data.\"\"\"\n",
    "    # extract the name of each directory\n",
    "    quarters = [d.name for d in DATA_DIR.iterdir() if d.is_dir()]\n",
    "    \n",
    "    return sorted(quarters)\n",
    "\n",
    "print('Available quarters:', list_quarters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading quarter data\n",
    "It is natural to analyze the data in a quarterly fashion.  \n",
    "We will only read in parts of the data relevant to our analysis. In particular:\n",
    "- INFOTABLE.tsv: `ACCESSION_NUMBER`, `INFOTABLE_SK`, `NAMEOFISSUER`, `CUSIP`, `FIGI`, `VALUE`, `SSHPRNAMT`, `SSHPRNAMTTYPE`, `PUTCALL`\n",
    "- COVERPAGE.tsv: `ACCESSION_NUMBER`, `REPORTCALENDARORQUARTER`, `FILINGMANAGER_NAME`\n",
    "- SUBMISSION.tsv: `ACCESSION_NUMBER`, `FILING_DATE`, `SUBMISSION_TYPE`, `PERIODOFREPORT`\n",
    "\n",
    "There is an important _pre-processing step_ that occurs here. The SEC made more stringent reporting standards for Form 13F effective 1/3/2023.  \n",
    "In particular, the dollar values before that date were reported to the closest thousand, whereas after that date they are reported to the closest dollar.\n",
    "\n",
    "To make appropriate comparisons between quarters and understand the dollar values, we must normalize the pre-2023 dollar amounts by multiplying the raw amounts by 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping from quarter string to expected PERIODOFREPORT date\n",
    "QUARTER_TO_PERIOD_PRE2024 = {\n",
    "    'q1': '-12-31',\n",
    "    'q2': '-03-31',\n",
    "    'q3': '-06-30',\n",
    "    'q4': '-09-30'\n",
    "}\n",
    "\n",
    "def exp_period(quarter: str) -> pd.Timestamp:\n",
    "    \"\"\"Get the expected period of report date from a string representing year/quarter.\"\"\"\n",
    "    # Parse quarter string (e.g., '2023q4') to get expected period date\n",
    "    year = int(quarter[:4]) # e.g., 2023\n",
    "    q = quarter[4:]    # e.g., 'q4'\n",
    "\n",
    "    if year < 2024:\n",
    "        # if quarter is q1, expected period of filing lies in previous year\n",
    "        if q == 'q1':\n",
    "            year -= 1\n",
    "            \n",
    "        return pd.Timestamp(f'{year}-{QUARTER_TO_PERIOD_PRE2024[q]}')\n",
    "    else:\n",
    "        # TODO: adapt to parse this data\n",
    "        print('Error: parsing quarterly data from 2024 onwards is unsupported due to a change in the filing dates.')\n",
    "        raise ValueError\n",
    "\n",
    "    \n",
    "\n",
    "def load_quarter(quarter: str) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load holdings, filing, and amendment data for a specific quarter.\n",
    "    \n",
    "    Returns:\n",
    "        holdings_reports: 13F-HR filings for the quarter's period\n",
    "        filings: COVERPAGE data\n",
    "        amendment_reports: 13F-HR/A filings for the quarter's period\n",
    "    \"\"\"\n",
    "    quarter_dir = DATA_DIR / quarter\n",
    "\n",
    "    expected_period = exp_period(quarter)\n",
    "    \n",
    "    infotable_datatypes = {\n",
    "        'ACCESSION_NUMBER': 'string',\n",
    "        'INFOTABLE_SK': 'Int64',\n",
    "        'NAMEOFISSUER': 'string',\n",
    "        'CUSIP': 'string',\n",
    "        'FIGI': 'string',\n",
    "        'VALUE': 'Int64',\n",
    "        'SSHPRNAMT': 'Int64',\n",
    "        'SSHPRNAMTTYPE': 'string',\n",
    "        'PUTCALL': 'string',\n",
    "    }\n",
    "\n",
    "    filings_datatypes = {\n",
    "        'ACCESSION_NUMBER': 'string',\n",
    "        'REPORTCALENDARORQUARTER': 'string',\n",
    "        'FILINGMANAGER_NAME': 'string'\n",
    "    }\n",
    "\n",
    "    submissions_datatypes = {\n",
    "        'ACCESSION_NUMBER': 'string',\n",
    "        'FILING_DATE': 'string',\n",
    "        'SUBMISSIONTYPE': 'string',\n",
    "        'PERIODOFREPORT': 'string'\n",
    "    }\n",
    "\n",
    "    # read tables and rename columns for readability\n",
    "\n",
    "    holdings = pd.read_table(\n",
    "        quarter_dir / 'INFOTABLE.tsv',  \n",
    "        dtype=infotable_datatypes,\n",
    "        usecols=infotable_datatypes.keys()\n",
    "    ).rename(columns={'SSHPRNAMT': 'SHARES/PRINCIPAL AMOUNT', 'SSHPRNAMTTYPE': 'AMOUNT_UNIT', 'NAMEOFISSUER': 'COMPANY_NAME'})\n",
    "\n",
    "    filings = pd.read_table(\n",
    "        quarter_dir / 'COVERPAGE.tsv',\n",
    "        dtype=filings_datatypes,\n",
    "        usecols=filings_datatypes.keys(),\n",
    "        parse_dates=['REPORTCALENDARORQUARTER'],\n",
    "        date_format='%d-%b-%Y'\n",
    "    ).rename(columns={'FILINGMANAGER_NAME': 'MANAGER_NAME'})\n",
    "\n",
    "    raw_submissions = pd.read_table(\n",
    "        quarter_dir / 'SUBMISSION.tsv',\n",
    "        dtype=submissions_datatypes,\n",
    "        usecols=submissions_datatypes.keys(),\n",
    "        parse_dates=['FILING_DATE', 'PERIODOFREPORT'],\n",
    "        date_format='%d-%b-%Y'\n",
    "    )\n",
    "\n",
    "    # Filter to 13F-HR and 13F-HR/A submissions for the expected period only\n",
    "    relevant_submissions = raw_submissions[\n",
    "        (raw_submissions['SUBMISSIONTYPE'].isin(['13F-HR', '13F-HR/A'])) &\n",
    "        (raw_submissions['PERIODOFREPORT'] == expected_period)\n",
    "    ]\n",
    "\n",
    "    holdings = pd.merge(holdings, relevant_submissions, on=['ACCESSION_NUMBER'])\n",
    "\n",
    "    # Normalize VALUE: pre-2023 data reported in thousands, 2023+ in dollars\n",
    "    VALUE_CUTOFF_DATE = pd.Timestamp('2023-01-03')\n",
    "    pre_2023_mask = holdings['FILING_DATE'] < VALUE_CUTOFF_DATE\n",
    "    holdings.loc[pre_2023_mask, 'VALUE'] = holdings.loc[pre_2023_mask, 'VALUE'] * 1000\n",
    "\n",
    "    holdings['QUARTER'] = quarter\n",
    "\n",
    "    # Split into holding reports and amendments\n",
    "    is_amendment = holdings['SUBMISSIONTYPE'] == '13F-HR/A'\n",
    "\n",
    "    holdings_reports = holdings[~is_amendment]\n",
    "    amendment_reports = holdings[is_amendment]\n",
    "    \n",
    "    return holdings_reports, filings, amendment_reports\n",
    "\n",
    "\n",
    "# Load most recent quarter\n",
    "quarters = list_quarters()\n",
    "sample_holdings, sample_filings, sample_amendments = load_quarter(quarters[-1])\n",
    "print(f'Loaded {len(sample_holdings):,} holdings and {len(sample_amendments):,} amendments from {len(sample_filings):,} filers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_holdings.head().style.format(thousands=',', subset=['VALUE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_filings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnostics: Data Quality & Filing Integrity\n",
    "\n",
    "SEC bulk 13F datasets for a given quarter are not limited to a single, clean snapshot of holdings. They often include amendments, late filings, and other administrative records that can contaminate the true quarter-end view. Before running any financial or time-series analysis, we need to validate that the dataset represents what we think it does.\n",
    "\n",
    "In particular, we check for three common failure modes:\n",
    "\n",
    "- Cross-period contamination: verifying that holdings from other PERIODOFREPORT dates are not leaking into the quarter being analyzed.\n",
    "\n",
    "- Duplicate manager–security pairs: ensuring that each filer–stock combination appears only once after filtering, so aggregate values are not artificially inflated.\n",
    "\n",
    "- Non-holdings filings: confirming that we are only including true holdings filings (e.g., not notices, amendments, or other submission types that should not contribute to portfolio value).\n",
    "\n",
    "These diagnostics act as a guardrail: if any of them fail, the downstream valuation, aggregation, and trend analyses cannot be trusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic 1: Check PERIODOFREPORT distribution\n",
    "# For instance, the 2023q4 bulk file should only contain filings for 2023-09-30\n",
    "print('=== PERIODOFREPORT Distribution ===')\n",
    "period_counts = sample_holdings['PERIODOFREPORT'].value_counts()\n",
    "print(period_counts.head(15))\n",
    "\n",
    "if len(period_counts) != 1:\n",
    "    print('error: holdings contained more than one date')\n",
    "    raise ValueError\n",
    "\n",
    "print(f'\\nDate: {period_counts.index[0].date()} in {period_counts.iloc[0]:,} rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic 2: Check for duplicate manager/stock combinations within the quarter\n",
    "# Join with filings to get manager name, then group by manager + stock\n",
    "merged_holdings = pd.merge(sample_holdings, sample_filings, on='ACCESSION_NUMBER')\n",
    "dupes = merged_holdings.groupby(['MANAGER_NAME', 'COMPANY_NAME']).agg(\n",
    "    NUM_ACCESSIONS=('ACCESSION_NUMBER', 'nunique'),\n",
    "    TOTAL_VALUE=('VALUE', 'sum'),\n",
    "    NUM_ROWS=('VALUE', 'count')\n",
    ").reset_index()\n",
    "\n",
    "# Filter to cases with more than one row per manager/stock\n",
    "multi_row = dupes[dupes['NUM_ROWS'] > 1].copy().sort_values('TOTAL_VALUE', ascending=False)\n",
    "print(f'=== Manager/Stock combinations with multiple rows ===')\n",
    "print(f'Total combinations with >1 row: {len(multi_row):,}')\n",
    "print(f'\\nTop 10 by total value:')\n",
    "multi_row.head(10).style.format(thousands=',', subset=['TOTAL_VALUE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic 3: Check SUBMISSIONTYPE distribution\n",
    "\n",
    "print('=== SUBMISSIONTYPE Distribution ===')\n",
    "submission_type_counts = sample_holdings['SUBMISSIONTYPE'].value_counts()\n",
    "print(submission_type_counts.head(15))\n",
    "\n",
    "if len(submission_type_counts) != 1:\n",
    "    print('error: holdings contained more than one submission type')\n",
    "    raise ValueError\n",
    "\n",
    "submission_type = submission_type_counts.index[0]\n",
    "if submission_type != '13F-HR':\n",
    "    print(f'error: holdings contained non-holdings submission type: {submission_type}')\n",
    "    raise ValueError\n",
    "\n",
    "print(f'\\nSubmission type: {submission_type_counts.index[0]} in {submission_type_counts.iloc[0]:,} rows')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic 4: Verify one accession per manager/stock (holdings only, excluding amendments)\n",
    "# Join holdings with filings to get manager names\n",
    "\n",
    "# Group by manager + stock and count unique accessions\n",
    "accession_check = merged_holdings.groupby(['MANAGER_NAME', 'COMPANY_NAME']).agg(\n",
    "    NUM_ACCESSIONS=('ACCESSION_NUMBER', 'nunique'),\n",
    "    TOTAL_VALUE=('VALUE', 'sum'),\n",
    "    NUM_ROWS=('VALUE', 'count')\n",
    ").reset_index()\n",
    "\n",
    "# Check for any groups with multiple accessions\n",
    "multi_accession = dupes[dupes['NUM_ACCESSIONS'] > 1]\n",
    "\n",
    "print(f'=== Accession Uniqueness Check (Holdings Only) ===')\n",
    "print(f'Total manager/stock groups: {len(dupes):,}')\n",
    "print(f'Groups with NUM_ACCESSIONS > 1: {len(multi_accession):,}')\n",
    "\n",
    "if len(multi_accession) > 0:\n",
    "    print(f'\\nTop 10 by value with multiple accessions:')\n",
    "    print(multi_accession.sort_values('TOTAL_VALUE', ascending=False).head(10))\n",
    "    raise ValueError('Found manager/stock groups with multiple accessions')\n",
    "else:\n",
    "    print('\\nAll manager/stock groups have exactly one accession number associated with them.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Holdings to AI Stocks\n",
    "\n",
    "13F filings identify securities by CUSIP. We'll match on issuer name to identify AI stocks.  \n",
    "This is where the dictionary of AI stock tickers will prove useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_ai_stock_to_ticker(issuer_name: str) -> str | None:\n",
    "    \"\"\"Map issuer name to stock ticker.\"\"\"\n",
    "    if pd.isna(issuer_name):\n",
    "        return None\n",
    "    \n",
    "    issuer_upper = issuer_name.upper()\n",
    "    \n",
    "    # prioritize an exact match over a substring match\n",
    "    if issuer_upper in ai_name_to_ticker:\n",
    "        return ai_name_to_ticker[issuer_upper]\n",
    "    \n",
    "    for company_name, ticker in ai_name_to_ticker.items():\n",
    "        if issuer_upper in company_name or company_name in issuer_upper:\n",
    "            return ticker\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the `filter_to_ai_stocks` function to:\n",
    "1. Add an extra column to the DataFrame, corresponding to the stock ticker of the relevant AI stock or `None` if the stock indicated is not an AI stock\n",
    "2. Given the augmented `sample_holdings` table, filter to only the entries with AI stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_to_ai_stocks(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds TICKER and AI_TYPE columns to the dataframe and returns a new \n",
    "    DataFrame representing only the entries related to AI stocks.\n",
    "    \"\"\"\n",
    "\n",
    "    if 'COMPANY_NAME' not in df.columns.values:\n",
    "        print('Error: company name not found in columns of dataframe.')\n",
    "        raise LookupError\n",
    "    \n",
    "    df['TICKER'] = df['COMPANY_NAME'].apply(match_ai_stock_to_ticker)\n",
    "    filtered = df.dropna(subset=['TICKER']).copy()\n",
    "    filtered['AI_TYPE'] = filtered['TICKER'].map(ticker_to_ai_type)\n",
    "    return filtered\n",
    "\n",
    "# Apply matcher and filter\n",
    "sample_ai_holdings = filter_to_ai_stocks(sample_holdings)\n",
    "\n",
    "print(f'Found {len(sample_ai_holdings):,} filings related to AI holdings ({len(sample_ai_holdings)/len(sample_holdings)*100:.1f}% of total)')\n",
    "ai_pct_value = sample_ai_holdings['VALUE'].sum()/sample_holdings['VALUE'].sum()\n",
    "print(f'AI holdings are {ai_pct_value:.3f}% of total portfolio value')\n",
    "\n",
    "sample_ai_holdings.head().style.format(thousands=',', subset=['VALUE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate by Stock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first aggregate by stock to see which stocks have the highest value in terms of institutional investment value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total institutional holdings by AI stock\n",
    "def aggregate_by_stock(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.groupby('TICKER').agg({'VALUE': 'sum', 'COMPANY_NAME': 'first', 'CUSIP': 'first'}).sort_values('VALUE', ascending=False)\n",
    "\n",
    "agg_ai_by_stock = aggregate_by_stock(sample_ai_holdings)\n",
    "agg_ai_by_stock.head(10).style.format(thousands=',', subset=['VALUE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate holdings by fund\n",
    "We can also understand which institutional investors are pursuing AI-related stocks the most. We will aggregate by filers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_by_manager(holdings: pd.DataFrame, filings: pd.DataFrame) -> pd.DataFrame:\n",
    "    return pd.merge(holdings, filings, on=['ACCESSION_NUMBER']).groupby('MANAGER_NAME').agg({'VALUE': 'sum'}).sort_values('VALUE', ascending=False)\n",
    "\n",
    "agg_ai_by_fund = aggregate_by_manager(sample_ai_holdings, sample_filings)\n",
    "agg_ai_by_fund.head(15).style.format(thousands=',', subset=['VALUE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate by AI Category\n",
    "We can see which types of AI stocks are most popular with institutional investors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_by_ai_type(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.groupby('AI_TYPE').agg({\n",
    "        'VALUE': 'sum',\n",
    "        'TICKER': 'nunique',\n",
    "        'ACCESSION_NUMBER': 'nunique'\n",
    "    }).rename(columns={'TICKER': 'NUM_STOCKS', 'ACCESSION_NUMBER': 'NUM_MANAGERS'}).sort_values('VALUE', ascending=False)\n",
    "\n",
    "agg_by_type = aggregate_by_ai_type(sample_ai_holdings)\n",
    "agg_by_type.style.format(thousands=',', subset=['VALUE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Analysis\n",
    "Load quarters from 2022 Q3 onward to analyze AI stock holdings trends during the AI boom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_quarters_from(start_quarter: str) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Load and concatenate quarters starting from start_quarter.\"\"\"\n",
    "    all_quarters = list_quarters()\n",
    "    target_quarters = [q for q in all_quarters if q >= start_quarter]\n",
    "    \n",
    "    all_ai_holdings = []\n",
    "    all_filings = []\n",
    "    \n",
    "    for q in target_quarters:\n",
    "        holdings, filings, _ = load_quarter(q)\n",
    "        ai_holdings = filter_to_ai_stocks(holdings)\n",
    "        \n",
    "        all_ai_holdings.append(ai_holdings)\n",
    "        all_filings.append(filings)\n",
    "        print(f'Loaded {q}: {len(ai_holdings):,} AI holdings')\n",
    "    \n",
    "    return pd.concat(all_ai_holdings, ignore_index=True), pd.concat(all_filings, ignore_index=True)\n",
    "\n",
    "ai_holdings_ts, filings_ts = load_quarters_from('2022q3')\n",
    "print(f'\\nTotal: {len(ai_holdings_ts):,} AI holdings across {ai_holdings_ts[\"QUARTER\"].nunique()} quarters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holdings by AI Category Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_type_by_quarter = ai_holdings_ts.pivot_table(\n",
    "    index='QUARTER', \n",
    "    columns='AI_TYPE', \n",
    "    values='VALUE', \n",
    "    aggfunc='sum',\n",
    "    fill_value=0\n",
    ")\n",
    "ai_type_by_quarter.style.format(thousands=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top AI Stocks Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarterly_stock = ai_holdings_ts.groupby(['QUARTER', 'TICKER'])['VALUE'].sum().reset_index()\n",
    "quarterly_stock['RANK'] = quarterly_stock.groupby('QUARTER')['VALUE'].rank(ascending=False, method='first').astype(int)\n",
    "\n",
    "# top 10 ranks per quarter\n",
    "top10_per_quarter = quarterly_stock[quarterly_stock['RANK'] <= 10].pivot(\n",
    "    index='RANK',\n",
    "    columns='QUARTER', \n",
    "    values='TICKER'\n",
    ")\n",
    "top10_per_quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dollar values for top 10 per quarter\n",
    "top10_values = quarterly_stock[quarterly_stock['RANK'] <= 10].pivot(\n",
    "    index='RANK',\n",
    "    columns='QUARTER',\n",
    "    values='VALUE'\n",
    ")\n",
    "top10_values.style.format(thousands=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This analysis demonstrates:\n",
    "- Loading SEC 13F bulk data and matching to AI stocks\n",
    "- Identifying AI stocks with the highest institutional holdings\n",
    "- Aggregating by AI category (semiconductors, cloud, etc.)\n",
    "- Calculating funds with the greatest AI stock holdings\n",
    "- Tracking top stocks per quarter across the AI boom (2022q3+)\n",
    "\n",
    "**Extensions:**\n",
    "- Calculating AI stock value as percentage of total portfolio value\n",
    "- Analyzing behavior around key events (e.g. ChatGPT launch, Gemini launch)\n",
    "- Identifying funds increasing/decreasing AI exposure over time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
